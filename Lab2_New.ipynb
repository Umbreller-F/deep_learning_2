{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imdb import *\n",
    "from utils import *\n",
    "from config import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../\\.data/imdb\"\n",
    "imdb_reviews = IMDBMovieReviews(path)\n",
    "train_data, test_data = imdb_reviews.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, dev_data = imdb_reviews.split_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in (train_data, dev_data, test_data):\n",
    "    imdb_reviews.tokenize(data, max_seq_len=MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = \"../\\.vector_cache/glove.6B.100d.txt\"\n",
    "glove = GloVeWordEmbeddings(glove_path, int((glove_path.split(\".\")[-2]).split(\"d\")[0]))\n",
    "\n",
    "token_to_index_mapping = imdb_reviews.create_vocab(train_data, unk_threshold=UNK_THRESHOLD)\n",
    "\n",
    "token_to_glove_mapping = glove.get_token_to_embedding()\n",
    "indices_found, embedding_matrix = imdb_reviews.get_embeds(token_to_index_mapping, token_to_glove_mapping, glove.get_num_dims())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_idx = {\"neg\": 0, \"pos\": 1}\n",
    "for data in (train_data, dev_data, test_data):\n",
    "    imdb_reviews.apply_vocab(data, token_to_index_mapping)\n",
    "    imdb_reviews.apply_label_map(data, label_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx = token_to_index_mapping[PAD]\n",
    "train_dataset = SentimentDataset(train_data, pad_idx)\n",
    "dev_dataset = SentimentDataset(dev_data, pad_idx)\n",
    "test_dataset = SentimentDataset(test_data, pad_idx)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=train_dataset.collate_fn\n",
    ")\n",
    "dev_dataloader = DataLoader(\n",
    "    dev_dataset, batch_size=BATCH_SIZE, collate_fn=dev_dataset.collate_fn\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, collate_fn=test_dataset.collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_labels, n_rnn_layers, pad_idx, embedding_matrix, freeze=True):\n",
    "        super().__init__()\n",
    "        self.pad_idx = pad_idx\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)# , padding_idx=pad_idx\n",
    "        # self.embedding.weight.requires_grad = False\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "        \n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=n_rnn_layers,\n",
    "            batch_first=True, \n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        # self.output = nn.Linear(hidden_dim, n_labels)\n",
    "        self.fc1 = nn.Linear(hidden_dim*2, 16*2)\n",
    "        self.dropout = nn.Dropout(0.15)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(16*2, n_labels)\n",
    "        # self.h = torch.zeros(1, BATCH_SIZE, hidden_dim).cuda()\n",
    "\n",
    "    def forward(self, text):\n",
    "        x = self.embedding(text)\n",
    "        a, x = self.rnn(x)\n",
    "        # print(a, x)\n",
    "        # print(x.shape)\n",
    "        output_f = x[-2, :, :]\n",
    "        output_b = x[-1, :, :]\n",
    "        x = torch.cat([output_f, output_b], dim=-1)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_labels, pad_idx, embedding_matrix, freeze=True):\n",
    "        super().__init__()\n",
    "        self.pad_idx = pad_idx\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)# , padding_idx=pad_idx\n",
    "        # self.embedding.weight.requires_grad = False\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim, \n",
    "            hidden_size=hidden_dim, \n",
    "            num_layers=2,\n",
    "            batch_first=True, \n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        # self.output = nn.Linear(hidden_dim, n_labels)\n",
    "        self.fc1 = nn.Linear(hidden_dim*2, 16*2)\n",
    "        self.dropout = nn.Dropout(0.15)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(16*2, n_labels)\n",
    "        # self.h = torch.zeros(1, BATCH_SIZE, hidden_dim).cuda()\n",
    "\n",
    "    def forward(self, text):\n",
    "        x = self.embedding(text)\n",
    "        a, (x, _) = self.lstm(x)\n",
    "        # print(a, x)\n",
    "        # print(x.shape)\n",
    "        output_f = x[-2, :, :]\n",
    "        output_b = x[-1, :, :]\n",
    "        x = torch.cat([output_f, output_b], dim=-1)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_labels, pad_idx, embedding_matrix, freeze=True):\n",
    "        super().__init__()\n",
    "        self.pad_idx = pad_idx\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)# , padding_idx=pad_idx\n",
    "        # self.embedding.weight.requires_grad = False\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=embedding_dim, \n",
    "            hidden_size=hidden_dim, \n",
    "            num_layers=2,\n",
    "            batch_first=True, \n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        # self.output = nn.Linear(hidden_dim, n_labels)\n",
    "        self.fc1 = nn.Linear(hidden_dim*2, 16*2)\n",
    "        self.dropout = nn.Dropout(0.15)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(16*2, n_labels)\n",
    "        # self.h = torch.zeros(1, BATCH_SIZE, hidden_dim).cuda()\n",
    "\n",
    "    def forward(self, text):\n",
    "        x = self.embedding(text)\n",
    "        _, x = self.gru(x)\n",
    "        output_f = x[-2, :, :]\n",
    "        output_b = x[-1, :, :]\n",
    "        x = torch.cat([output_f, output_b], dim=-1)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "rnn1 = RNN(vocab_size=embedding_matrix.shape[0], embedding_dim=100, hidden_dim=256, n_labels=2, n_rnn_layers=1, \n",
    "            pad_idx=pad_idx, embedding_matrix=embedding_matrix)\n",
    "\n",
    "rnn2 = RNN(vocab_size=embedding_matrix.shape[0], embedding_dim=100, hidden_dim=256, n_labels=2, n_rnn_layers=2, \n",
    "            pad_idx=pad_idx, embedding_matrix=embedding_matrix)\n",
    "\n",
    "lstm = LSTM(vocab_size=embedding_matrix.shape[0], embedding_dim=100, hidden_dim=128, n_labels=2, \n",
    "            pad_idx=pad_idx, embedding_matrix=embedding_matrix)\n",
    "\n",
    "gru = GRU(vocab_size=embedding_matrix.shape[0], embedding_dim=100, hidden_dim=64, n_labels=2, \n",
    "            pad_idx=pad_idx, embedding_matrix=embedding_matrix)\n",
    "'''\n",
    "\n",
    "model = RNN(vocab_size=embedding_matrix.shape[0], embedding_dim=100, hidden_dim=64, n_labels=2, n_rnn_layers=2, \n",
    "            pad_idx=pad_idx, embedding_matrix=embedding_matrix)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, epochs):\n",
    "    correct_train=0\n",
    "    num_train=0\n",
    "    ac_train=[]\n",
    "    ac_validate=[]\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        correct_train=0\n",
    "        num_train=0\n",
    "        print('epoch:{}'.format(epoch))\n",
    "        for batch, (X, Y)  in enumerate(train_dataloader):\n",
    "            # torch.cuda.empty_cache()\n",
    "            # print('batch:{}'.format(batch))\n",
    "            X = X.cuda()\n",
    "            Y = Y.cuda()\n",
    "            # print(X.shape,Y.shape)\n",
    "            # if X.shape[0]<128:\n",
    "            #     print('not enough!')\n",
    "            #     break\n",
    "            num_train+=X.shape[0]\n",
    "            # print(torch.unsqueeze(X, dim=0).shape)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(X)\n",
    "            out = torch.squeeze(out)\n",
    "            \n",
    "            # print(out.shape, Y.shape)\n",
    "            # print(torch.squeeze(out).shape)\n",
    "            # print(out.shape)\n",
    "            loss = criterion(out, Y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            pred = out.argmax(dim=1)\n",
    "            # print(pred)\n",
    "\n",
    "            correct_train+=pred.eq(Y).sum()\n",
    "        scheduler.step()\n",
    "        ac_train.append(float(correct_train)/float(num_train))\n",
    "\n",
    "        model.eval()\n",
    "        correct_validate=0\n",
    "        num_validate=0\n",
    "        with torch.no_grad():\n",
    "            for batch, (X, Y)  in enumerate(dev_dataloader):\n",
    "                X = X.cuda()\n",
    "                Y = Y.cuda()\n",
    "                num_validate+=X.shape[0]\n",
    "                out = model(X)\n",
    "                out = torch.squeeze(out)\n",
    "                pred = out.argmax(dim=1)\n",
    "                # print(pred)\n",
    "\n",
    "                correct_validate+=pred.eq(Y).sum()\n",
    "                loss = criterion(out, Y)\n",
    "                # optimizer.step()\n",
    "\n",
    "        ac_validate.append(float(correct_validate)/float(num_validate))\n",
    "        print(ac_train, ac_validate)\n",
    "    \n",
    "    return ac_train, ac_validate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'测试集'\n",
    "def test(model_to_test):\n",
    "    correct_test=0\n",
    "    num_test=0\n",
    "    # ac_validate=[]\n",
    "    model_to_test.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, Y)  in enumerate(test_dataloader):\n",
    "            X = X.cuda()\n",
    "            Y = Y.cuda()\n",
    "            num_test+=X.shape[0]\n",
    "            out = model_to_test(X)\n",
    "            out = torch.squeeze(out)\n",
    "            pred = out.argmax(dim=1)\n",
    "            # print(pred)\n",
    "\n",
    "            correct_test+=pred.eq(Y).sum()\n",
    "            # loss = criterion(out, Y)\n",
    "            # optimizer.step()\n",
    "    print(float(correct_test)/float(num_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_train,ac_validate=fit(model,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(10)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(ac_train)\n",
    "plt.plot(ac_validate)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(('Training', 'Validation'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model0.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = 'model0.pkl'\n",
    "model_copy = torch.load(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.67284\n"
     ]
    }
   ],
   "source": [
    "test(model_copy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
